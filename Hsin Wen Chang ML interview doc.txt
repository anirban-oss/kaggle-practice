Hsin-Wen Chang ML interview questions:

Dec 13, 22
 What is the difference between Online and Offline (Batch) learning?
 
 https://www.kaggle.com/general/317514
 
 Dec 14, 22
 What is the difference between Stochastic Gradient Descent (SGD) and standard Gradient Descent (GD) ü•∏ ?
 
 https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/
 https://www.geeksforgeeks.org/gradient-descent-algorithm-and-its-variants/
 
 Dec 15, 22
 Can backpropagation work well with multiple hidden layers ü§î ?
 
 Margaret Gathoni
 ‚Äî 
Today at 1:48 PM
My thoughts: Backpropagation is a good tool for improving the accuracy of prediction, however it's slow, hence with multiple hidden layers means more training time.

Hsin-Wen Chang
 ‚Äî 
Today at 3:11 PM
Good answer @Margaret Gathoni  ü•≥ ! Especially with sigmoid activate function the backpropagation doesn't usually work well if we have a lot of hidden layers as the diffusion of gradients leads to slow training in the lower layers. 

 Dec16, 22
 What do you understand by Rectified Linear Units (ReLU) ?
 
 The Rectified Linear Unit (ReLU) is the most commonly used activation function in deep learning. The function returns 0 if the input is negative, but for any positive input, it returns that value back.
 
 https://towardsdatascience.com/why-rectified-linear-unit-relu-in-deep-learning-and-the-best-practice-to-use-it-with-tensorflow-e9880933b7ef
 
 https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/
 
 Dec17, 22 and Dec18, 22 is off days.
 
 Dec19, 22 
 Why is the softmax function used for the output layer ü§î ?

Softmax function is used for the output layer only (at least in most cases) to ensure that the sum of the components of output vector is equal to 1 (for clarity see the formula of softmax cost function). This also implies what is the probability of occurrence of each component (class) of the output and hence sum of the probabilities(or output components) is equal to 1.

Dec20, 22
Name some of the regularization methods that could be used in Artificial Neural Networks. ü§î

L1 / L2, Weight Decay, Dropout, Batch Normalization, Data Augmentation and Early Stopping

https://medium.com/yottabytes/a-quick-guide-on-basic-regularization-methods-for-neural-networks-e10feb101328

Dec21, 22
Describe Convolutional Neural Networks (CNNs) what are 4 building blocks that CNNs based on ü§î ? 

For the building block CNN basics are: a convolutional layer used to to detect features in an image, a pooling layer to help fix distorted images, and a fully connected layer to process the data in a neural network. It also has a Flattening layer to turn the image into a suitable representation. I also think a ReLU is crucial building block since it  make the image smooth and make boundaries distinct.

Hsin-Wen Chang
 ‚Äî 
Today at 3:06 PM
Good approach @Margaret Gathoni  üåü The primary purpose of convolution is to extract the features from input image using a small matrix call filter or kernel slides over the image and the dot product (feature map) is computed. By varying the filter, we can have different result like blur, edge detection. Pooling or sub sampling such as spatial pooling (downsampling) reduce dimension of each feature map. Downsampled feature maps get distorted. This distortion provides a way for data augmentation to improve the generality of the network. pool layer doesn't fix distorted image it downsampled feature maps get distorted image perform data augmentation to improve the network. Fully connected layer is use these features for classifying the input image into various classes base on training dataset.

https://cs231n.github.io/convolutional-networks/

Dec22, 22
Tell me about Recurrent Neural Networks (RNNs) ü§î ?

From my understanding they are a class/group of artificial neural networks which are mainly used in NLP and speech recognition. They use the same principal of interconnected group of nodes just like a human brain.

Good answer @Margaret Gathoni ü•≥ , RNNs is to make use of the sequential information. They perform the same task for every element of a sequence with the output depended on the previous computation that's why call recurrent. unlike traditional neural networks, RNNs have loop in them allow information to persist.

Dec23, 22
If the weights oscillate a lot over training iterations ( swinging between positive and negative values) what parameter do you need to tune to address this issue ü§î ?

Is this the instance where you adjust your learning rate, max and min depth, the estimator and finally the subsample which controls the data set samples that each iteration uses?

Good approach üí™  @Margaret Gathoni Tuning learning rate is the correct answer. If learning rate is too high it will cause the result to jump over the optimal point resulting in the weights oscillating between positive and negative. If it's too low, it may take a very long time to converge.

Dec26, 22
What is Regularization ü§î ?

Regularization is a method to constraint the model to fit our data accurately and not overfit.

Dec27, 22
What is the difference between the L-1 and L-2 regularization ü§î ?

Margaret Gathoni
 ‚Äî 
Yesterday at 2:31 PM
L1 tends to shrink all  coefficients by the same factor and some are shrank to zero mostly resulting in a sparse model; L2 on the other hand also shrinks the coefficient using same factors but none of them are eliminated hence no sparse model.
Hsin-Wen Chang
 ‚Äî 
Yesterday at 3:17 PM
Good answer @Margaret Gathoni ü•≥ ! L1 regularization minimizes the sum of absolute errors and thus, used for feature selection in sparse feature space by making few coefficients zero. L2 minimizes the sum of squares of absolute errors used to smoothen the solution by creating small distributed coefficients. L1 is more robust since it's resistant to the outliers. L2 squares the error so for any outlier its square term would be huge.
Margaret Gathoni
 ‚Äî 
Yesterday at 3:24 PM
It's even more clear why L1 is more robust on the last statement. Thank you

Dec28, 22
What is the difference between density sparse data and dimensionally sparse data üßê ?

Margaret Gathoni
 ‚Äî 
Today at 1:07 PM
dense sparse means most of the values are not zero while dimensional sparse most features have zero values.
Hsin-Wen Chang
 ‚Äî 
Today at 1:46 PM
Hello @Margaret Gathoni, density sparse data means that a high percentage of the data contains 0 or null value while dimensionally sparse data has large feature space in which some of the features are redundant, correlated etc.
Margaret Gathoni
 ‚Äî 
Today at 1:54 PM
This is better explained. Thank you

Dec29, 22
Is it beneficial to perform dimensionality reduction before fitting an SVM? Why or why not üßê ?

My guess is yes it's beneficial. Dimensional reduction as a form of feature scaling is important since distance between two features that are scaled to those that aren't differ significantly and especially for a sensitive model like svm. Therefore, standardizing the feature values improves the classifier performance significantly.

Hsin-Wen Chang
 ‚Äî 
Yesterday at 2:48 PM
Good attempt @Margaret Gathoni ü§ì ! Reducing the number of features will definitely reduce the computational complexity of the model but it may not improve the performance of SVM since SVM already use regularization to prevent overfitting thus performing dimensionality reduction before SVM may not improve the performance of the SVM classifier.

Dec30, 22
Name some of the evaluation metrics you know. Something apart from accuracy ü§î ?

Daisy Chelangat
 ‚Äî 
Today at 12:58 PM
I guess there is also recall and precision
PhilomenaMbura
 ‚Äî 
Today at 12:58 PM
F1 score
Hsin-Wen Chang
 ‚Äî 
Today at 1:04 PM
Good approach @Daisy Chelangat ü•≥  recall and precision are measure of a model's accuracy and we want other evaluation metrics that apart from accuracy.

Hsin-Wen Chang
 ‚Äî 
Today at 1:06 PM
Good approach üí™ @PhilomenaMbura ! F1 score is also a measure of model's accuracy and we want other evaluation metrics that apart from accuracy üßê .
Timothy Musungu
 ‚Äî 
Today at 1:27 PM
How about MSE and MAE?
Margaret Gathoni
 ‚Äî 
Today at 1:29 PM
RMSE

Hsin-Wen Chang
 ‚Äî 
Good answer @Margaret Gathoni ü•≥ ! Other evaluation metrics such as:
1. logarithmic loss.
2. Area under ROC curve.
3. Mean squared error (MSE)/Root Mean Square Error(RMSE) and mean absolute error (MAE) in regression.
Margaret Gathoni
 ‚Äî 
Today at 5:36 PM
Come to think of it i have never considered AUC as an evaluation metrics

Hsin-Wen Chang
 ‚Äî 
Today at 11:33 AM
It's very less use in reality most time we use precision, recall.

***many conversations***

Hsin-Wen Chang
 ‚Äî 
Yesterday at 12:22 PM
Hello @Margaret Gathoni,
I think I over simplify the use case  of the Precision-Recall AUC (PR AUC) vs ROC AUC before. The following is an helpful  detail reading like @Mensur Dlakic explained before:
https://ashutoshtripathi.com/2022/01/09/what-is-the-difference-between-precision-recall-curve-vs-roc-auc-curve/ 

Jan3, 23
How would you handle the scenario where your dataset has missing or dirty values üßê ?

For missing values you decide  depending on the data set and type of analysis you will be doing. do a a skewness test if you want to fill the missing value. The skewness test help you to know whether to fill with mean, median or mode. You can also decide to backfill or forward fill. for cleaning of data set where dealing with null values is also a part of you do validity check, accuracy check, completeness check, consistency and uniformity check. My thoughts.

Jan4, 23
How would you differentiate supervised and unsupervised learning ü§î ?

Margaret Gathoni
 ‚Äî 
Today at 1:50 PM
Supervised learning uses labelled data and is used to classify data or make predictions while unsupervised learning doesn't use labelled data and is used to understand relationship with in the dataset.
Hsin-Wen Chang
 ‚Äî 
Today at 2:10 PM
Good answer @Margaret Gathoni ü•≥ ! Supervised learning can be classified as a classification or a regression technique. Unsupervised learning is to model the distribution of the data. Unsupervised learning techniques include clustering, anomaly detection, and dimensionality reduction.

Jan5, 23
What do you mean by information gain ü§ì ?

It's a measure for uncertainty or randomness of data (entropy). Mostly uses in decision trees or random forest to decide the best split. I don't know how to phrase it better

It is an excellent answer üëè ü•≥ @Margaret Gathoni ! The information Gain is the change in the entropy.  Decision Tree determine the root node by calculate the entropy for each variable and its potential splits. Random forest is used in Ensembling or using averages of multiple models prevent overfitting with decision tree.

Jan6, 23
Describe some of the splitting rules used by different decision tree algorithms üßê ?

Margaret Gathoni
 ‚Äî 
Today at 12:21 PM
Is this where if you dealing with continuous variable in regression case you aim on variance reduction. And if you dealing with categorical variables like in classification cases you aim on Gini index , information gain and at times the Chi-square
Hsin-Wen Chang
 ‚Äî 
Today at 3:27 PM
Good answer @Margaret Gathoni üí™ ! it can separate  as the following:
Continuous variable:
* Reduction in Variance
Categorical variable:
* Gini Impurity
* Information Gain
* Chi-Square 

Jan9, 23
Our Machine Learning interview question for the day: Is using an ensemble like random forest always good üßê ?

Margaret Gathoni
 ‚Äî 
Today at 12:46 PM
I think it depends on the case at hand. Ensemble are better since they improve prediction however, they are at times hard to interpret.
'Toba Adesugba
 ‚Äî 
Today at 2:24 PM
I'd say it's not always good. Sometimes ensemble can be overkill for a simple dataset when a normal algorithm could have got the job done with the same amount of accuracy.

Also sometimes you may not have the time and resources that ensemble models require to perform training.
Fauzan Mohammed
 ‚Äî 
Today at 2:49 PM
A machine learning model can frequently perform better when an ensemble method like the random forest is used, but it is not always the optimal option. Unfortunately, they can also be more computationally expensive and may not be required if a single model performs well enough.
Hsin-Wen Chang
 ‚Äî 
Today at 3:48 PM
Excellent answers @Margaret Gathoni @'Toba Adesugba @Fauzan Mohammed ü•≥ !  Ensembles generally don't perform well when the relationship between dependent and independent variables is highly linear. The classification made by Random Forests is difficult to interpret easily unlike decision trees.

Jan10, 23
What is Ensemble Learning ü§î 

It's where you use multiple models for example bagging, stacking, and boosting. You combine the models to come up with improved results for you final model

Outstanding answer @Margaret Gathoni ü•≥ ! Comparing to  the standard machine learning approach Ensemble Learning combines a set hypotheses for prediction to boosts the weak learners hence improves the overall prediction accuracy. 

Jan11, 23
what are autoencodersü§î ?

Margaret Gathoni
 ‚Äî 
Yesterday at 1:41 PM
ANN used to learn efficient coding for unlabelled data. Utilized in unsupervised learning
Aayushi Jha
 ‚Äî 
Yesterday at 1:45 PM
Autoencoders are a type of feed forward neural networks which are trained to reproduce their input at the output layer. It consists of two parts, an encoder and decoder along with a hidden representation layer in between. 

Hsin-Wen Chang
 ‚Äî 
Yesterday at 4:01 PM
Good answer @Aayushi Jha @Margaret Gathoni üí™ ! In here encoding (converting the higher dimensional input to a much lower dimension hidden layers), decoding (converting the hidden layers to the output).  Autoencoders try to learn the approximation to the input and not actually predict any output it's useful as they find the low dimensional representation of the given dataset also remove any redundancy present in it.

Jan12, 23
What do you understand by Type I and Type II errors ü§î ?

Margaret Gathoni
 ‚Äî 
Today at 2:03 PM
Type 1 is False Positive which Type II is False Negative. Normally I interpret this using the justice example where: Type 1 (False Positive)- it's when the null hypothesis is rejected (Innocent until proven guilty) when the null  is actually true (defendant is actually innocent but ruled out as guilty). Type II (False Negative) - It's when the null hypothesis is not rejected when it's actually False to the population (defendant is actually guilty but ruled out as innocent).

Jan13, 23
What is a confusion matrix? Explain it for a 2-class problem ü§î ?

anirban chatterjee
 ‚Äî 
Today at 5:25 PM
A matrix which is used to figure out the accuracy of classification models. In this matrix, you're able to see both the predicted values positioned next to the actual outcomes for the datasets.
When computing two-classification problems, you can use this matrix to find:

    Accuracy rate: This is the percentage of correct predictions within a dataset.

    Misclassification rate: This is the percentage of times a classifier is incorrect.

    True positive rate: Also referred to as sensitivity, this allows researchers to identify correctly all the data which falls within a specific classification.

    True negative rate: Also referred to as specificity, and refers to how often a classifier predicts an undesirable outcome.

    False-positive rate: This represents how often a classifier is incorrect when predicting desirable outcomes.

    False-negative rate: This is an error that represents how often a classifier is incorrect when predicting undesirable outcomes.

    Decision rate: This is the rate at which desirable predictions turn out to be correct.
Margaret Gathoni
 ‚Äî 
Today at 5:27 PM
A table used to illustrate algorithm performance using metric such as Recall, Precision, Accuracy, F1-score etc. Normally we're checking to see our model ability to identify True positive, True Negative and quantify what it classify as False Negative and False positive

Jan16, 23
Explain the Bias-Variance trade-off ü•∏

Margaret Gathoni
 ‚Äî 
Today at 2:00 PM
the bias‚Äìvariance tradeoff is the property that increases the bias in an estimator can reduce the variance across samples of the parameter estimated.
Hsin-Wen Chang
 ‚Äî 
Today at 4:51 PM
Good answer @Margaret Gathoni ü•≥ üéâ !
anirban chatterjee
 ‚Äî 
Today at 4:52 PM
Models that have¬†high bias tend to have¬†low variance. For example, linear regression models tend to have high bias (assumes a simple linear relationship between explanatory variables and response variable) and low variance (model estimates won‚Äôt change much from one sample to the next).

However, models that have low bias tend to have high variance. For example, complex non-linear models tend to have low bias (does not assume a certain relationship between explanatory variables and response variable) with high variance (model estimates can change a lot from one training sample to the next).

Jan17, 23
When would you use k-Nearest Neighbors for regression üßê ?

Margaret Gathoni
 ‚Äî 
Today at 1:01 PM
when the relationship between the independent and dependent variables is non-linear

Hsin-Wen Chang
 ‚Äî 
Today at 5:33 PM
Good answer @Margaret Gathoni ü•≥ ! We use K-NN in regression for estimating the continuous variables such as compute the Euclidean distance from the test instance to the labeled instances or order the labeled instances by increasing distance or find a heuristically optimal k nearest neighbors base on RMSE or calculate an inverse distance weighted average with the k-nearest multivariate neighbors.

Jan18, 23
What do you mean by curse of dimensionality ü§î ?

Margaret Gathoni
 ‚Äî 
Today at 2:19 PM
The challenges a machine learning algorithm faces when working with higher dimensions of data. Normally the  number of variables required to effectively characterize a model increases  as its dimensions (or characteristics) increase.
Aayushi Jha
 ‚Äî 
Today at 2:29 PM
Number of input variables or features in a data set are referred to as a dataset's dimensionality.
The curse of dimensionality refers to the fact that when we use large number of dimensions or have high dimensionality it can cause poor performance and overfitting in our ml model.
anirban chatterjee
 ‚Äî 
Today at 2:57 PM
The curse of dimensionality basically means that the error increases with the increase in the number of features. It refers to the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions. A higher number of dimensions theoretically allow more information to be stored, but practically it rarely helps due to the higher possibility of noise and redundancy in the real-world data.
Hsin-Wen Chang
 ‚Äî 
Today at 3:53 PM
Excellent answers @Margaret Gathoni @Aayushi Jha @anirban chatterjee ü•≥ üéâ ! When the number of features is very large relative to the number of instances in the dataset, certain ML algorithms struggle to train effective models. The problem of having a huge feature space is the curse of dimensionality. 

Jan 19, 23
How can you check if the regression model fits the data well ü§ì ?

Margaret Gathoni
 ‚Äî 
Today at 2:48 PM
Normally look at the difference between observed values vs predicted value. The difference should be small and also unbiased
Timothy Musungu
 ‚Äî 
Today at 3:08 PM
One way would be to use metrics such as R2, MAE, RMSE. A very simple way would be to create a regression plot and check if target values lie close to the regression line.
Hsin-Wen Chang
 ‚Äî 
Today at 3:57 PM
Good answers @Margaret Gathoni @Timothy Musungu üéâü•≥  ! We can use the following statistics to test the model's fitness:
R-squared->Statistical measure of how close the data points are to the fitted regression line.
F-test -> Evaluate the null hypothesis that all regression coefficients are equal to zero versus the alternative hypothesis that at least one is not to identify the best model which fits the given dataset.
RMSE->square root of the variance of the residuals which measure the average deviation of the estimates from the observed value.

Jan20, 23
What is a limitation of R-squared? How do you adjust it üßê ?

Daisy Chelangat
 ‚Äî 
Today at 12:42 PM
One problem with R2 is that it increases with increased number of variables and this might not reflect the real picture of variability in your model. We can use adjusted R2 instead
Daisy Chelangat
 ‚Äî 
Today at 12:43 PM
trial
Hsin-Wen Chang
 ‚Äî 
Today at 12:56 PM
It's an excellent answer @Daisy Chelangat ü•≥ ! R -squared range between 0 and 1, where 0 indicating the proposed does not improve prediction over the mean model and 1 indicating the perfect prediction. The drawback of R -squared like @Daisy Chelangat said it can increase as the predictors are added the the regression model even they are not actually improving the model's fit. We can use adjusted R-squared for model where the number of predictors is greater than 1. It incorporates the model's degrees of freedom and decreases if the increase in model fit does not make up for the loss of degrees of freedom thus it only increases when the model fit is actually improved.
Margaret Gathoni
 ‚Äî 
Today at 12:58 PM
It doesn't measure how well sample data fits a distribution from a population with a normal distribution
Hsin-Wen Chang
 ‚Äî 
Today at 4:53 PM
Good way of thinking @Margaret Gathoni üëè ! R squared can't be use for checking normal distribution since it tells us the linear association between 2 random variables.

Jan23, 23
How would you evaluate a Logistic Regression model ü§î ?

Cynthia Barasa
 ‚Äî 
Today at 12:11 PM
To evaluate a logistic regression model, one can use several metrics, including accuracy, precision, recall, and F1 score. 

A confusion matrix can be used to understand the specific errors made by the model. 

Receiver Operating Characteristic (ROC) curve and the area under the curve (AUC) are also commonly used to evaluate the performance of a logistic regression model. 

An  appropriate evaluation metric will depend on the specific problem and requirements of the model.
Margaret Gathoni
 ‚Äî 
Today at 12:20 PM
Logistic regression evaluation it's a classification domain so we use a confusion matrix with best metrics being F1 score , you can also use Precision and Recall depending on  the case at hand . and also Accuracy even though you need to be careful with accuracy. ROC AUC can also be used to determine how much the model is capable of distinguishing between classes. 

Good answers @Cynthia Barasa , @Margaret Gathoni Since Logistic Regression is used to predict the probabilities we can use AUROC curve along with the confusion matrix to determine its performance. Another way we can use AIC (Akaike Information Criterion) such as the analogous metrics of adjusted R-squared in logistic regression is AIC. AIC is the measure of fit which penalizes a model for the number of model coefficients where we prefer a model with the minimum AIC value. Another way we prefer the model with lower deviance value. Deviance represents the goodness of fit for a model.

Jan24, 23
Which metric is generally used to evaluate the performance of a
Logistic Regression model ü§ì ?

Cynthia Barasa
 ‚Äî 
Today at 12:03 PM
A common metric used to evaluate the performance of a logistic regression model is the accuracy.

Accuracy is the proportion of correct predictions made by the model.

Other evaluation metrics like precision, recall, F1-score and ROC-AUC are also used to evaluate the performance of logistic regression models.
hannahkariuki
 ‚Äî 
Today at 12:14 PM
RMSE, Accracy 
Margaret Gathoni
 ‚Äî 
Today at 12:18 PM
AIC
Hsin-Wen Chang
 ‚Äî 
Today at 12:21 PM
Good answer @Margaret Gathoni @Cynthia Barasa  ü•≥ In here AIC is the most frequent, generally and widely used metric for evaluating Logistic Regression model's performance more detail reading: http://www.sthda.com/english/articles/38-regression-model-validation/158-regression-model-accuracy-metrics-r-square-aic-bic-cp-and-more/
https://builtin.com/data-science/what-is-aic but there are some special case AIC isn't that useful detail reading:https://www.displayr.com/how-to-interpret-logistic-regression-outputs/ @hannahkariuki RMSE is used for regression, not classification detail reading https://www.kdnuggets.com/2022/04/logistic-regression-classification.html https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide 

Jan25, 23
Explain K-Means Clustering and its objective ü§ì ?

Margaret Gathoni
 ‚Äî 
Today at 11:44 AM
K-mean clustering is an Unsupervised algorithm used for grouping(clustering) data into predetermined clusters. The algorithm main objective is to minimize the sum of the distances between data points in each cluster.
Chandrakala Gowda
 ‚Äî 
Today at 11:48 AM
The objective is to classify a test object based on the train set. Using K-number of neighbors(from the train set) that are closest/similar to the test point based on majority vote. It is an unsupervised learning. 
hannahkariuki
 ‚Äî 
Today at 12:16 PM
K-means clustering is an unsupervised algorithm which involves assigning k  clusters to each example.  The objective is to minimize differences (distance) within each cluster and maximize differences between clusters.
Hsin-Wen Chang
 ‚Äî 
Today at 1:44 PM
Good answers @Margaret Gathoni @Chandrakala Gowda ü•≥ , K-Means Clustering is used for handling a large number of data points which partition the dataset into k clusters such that each data point belongs to the cluster with the nearest mean. Good approach @hannahkariuki üí™ ! K-Means Clustering starts from some initial clusters then reassigns data points to k clusters to minimize the total penalty.

Jan26, 23
What would you do if your model is suffering from low bias and high variance? And why? ü§î

Margaret Gathoni
 ‚Äî 
Yesterday at 12:11 PM
low bias and high variance means there is overfitting. The model will overfit the target. It's able to perform accurately with our training data but on test data the model performs poorly. In this case I would resample the training data and see if the evaluation of the model changes and also instead of just having a training and test data set I would include a validation data set. Since our main aim is for the model to make sensible prediction from the unseen data. Bearing in mind there is no case where you can have zero of either bias or variance and that when bias-variance tradeoff comes in handy.
hannahkariuki
 ‚Äî 
Yesterday at 12:21 PM
When the model is suffering from low bias and high variance, it implies that the model is overfitting. To address this, I would either reduce the number of features that is, keep only important features or regularization if all features are important this will reduce magnitude of the parameters
Hsin-Wen Chang
 ‚Äî 
Yesterday at 2:17 PM
Good answers @Margaret Gathoni @hannahkariuki üöÄ ü•≥ ! In low bias and high variance situations, we can use bagging algorithms such as Random Forest to divide dataset into subset made with repeated randomized sampling. These samples are used to generate a set of models using a single learning algorithm. Later the model predictions are combined using voting (classification) or averaging (regression). We can also use regularization technique where the higher model coefficients get penalized such that lowering the model complexity. Also like @hannahkariuki said use top n features from the variable importance chart (The statistical significance of each variable or feature in the data with respect to its effect on the generated model).
Andrea Ciufo
 ‚Äî 
Yesterday at 2:21 PM
I definetly love @Hsin-Wen Chang  questions. They help me to refresh everyday the theory behind concepts that I don't use always on daily basis https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff

Jan27, 23
Why and where do you need to use Cluster analysis üò∂‚Äçüå´Ô∏è ?

Chandrakala Gowda
 ‚Äî 
Today at 11:48 AM
One possible application would be medical field. classification of tissue samples? or cell samples? based on the similarity of the samples for the diagnosis of tumors etc...
Faith Were
 ‚Äî 
Today at 11:54 AM
We use cluster analysis to gain important insights from data by observing what clusters the data points fall into when we apply a clustering algorithm to the data.
hannahkariuki
 ‚Äî 
Today at 11:59 AM
Cluster analysis is used for investigating the structure in data. The aim is in finding groups of objects with homogeneous properties out of heterogeneous large samples.
Hsin-Wen Chang
 ‚Äî 
Today at 12:06 PM
Good answers @Chandrakala Gowda @Faith Were @hannahkariuki  ü•≥ ! Cluster analysis can be used for the initial analysis of the dataset base on the different target attributes such as the data lacking the output labels we can use clustering technique find the class labels by grouping the input data instances into different clusters then assign a unique lable to each cluster. It has been used in a wide variety of applications such as customer segmentation for marketing, weather forecast, genetic study and the use case like @Chandrakala Gowda said.

Jan30, 23
List some of the Cluster analysis methods ü§î ?
hannahkariuki
 ‚Äî 
01/30/2023 12:01 PM
1. Hierarchical methods which starts with the finest possible partition and puts groups together step by step.
2. Partitioning methods which start from finest possible structure, compute the distance matrix for the clusters and join the clusters that have the smallest distance.
Margaret Gathoni
 ‚Äî 
01/30/2023 1:41 PM
1. Partitioning e.g K-means.
2. Heirarchical e.g agglomerative/ divisive 
3. Density eg DBSCAN
Mani Sarkar
 ‚Äî 
01/30/2023 2:28 PM
And these as well:
- t-SNE (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), t-SNE and clustering (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7561036/)
- UMAP (https://umap-learn.readthedocs.io/en/latest/), UMAP for clustering (https://umap-learn.readthedocs.io/en/latest/clustering.html) 
Hsin-Wen Chang
 ‚Äî 
01/30/2023 2:47 PM
Awesome answers @hannahkariuki @Margaret Gathoni @Mani Sarkar üëç !  Hierarchical clustering produce hierarchy of clusters by merge smaller clusters into larger ones or divide larger one into smaller ones. The merge or splitting cluster depends on the metric such as Euclidean distance, Manhattan distance, Hamming distance to measure the dissimilarity between the sets of data instances. K- means clustering is suitable for large number of data points and uses far lesser iterations than the Hierarchical clustering, it assigns the data points to k clusters so each data point belong to the cluster with the closest mean.

Jan31, 23
Differentiate between Partitioning method and Hierarchical method ü§î .

Margaret Gathoni
 ‚Äî 
Yesterday at 2:34 PM
For partitioning you set the number of clusters (k) before hand while for hierarchical  you don't but the model itself dictates the number of important clusters to return
Hsin-Wen Chang
 ‚Äî 
Yesterday at 3:55 PM
Good answer @Margaret Gathoni üëç ! A partition clustering is a division of the set of data objects into non-overlapping clusters thus each object is in exactly one cluster while hierarchical clustering is a set of nested clusters which are organized as a tree and is generally faster than hierarchical clustering. Another part like @Margaret Gathoni said hierarchical clustering does not require any input parameters while the partition clustering algorithms require the number of clusters to begin.

Feb1, 23
How do you select k for K-Means Clustering algorithm üòµ‚Äçüí´ ?

Vijaya Tatavarty
 ‚Äî 
Today at 12:39 PM
Using elbow method
Margaret Gathoni
 ‚Äî 
Today at 1:00 PM
The Silhouette Method using the silhouette score, Elbow method using Within-cluster sum of squares (WCSS),  Interclass-intraclass distance ratio method using the Calinski-Harabasz index,
Hsin-Wen Chang
 ‚Äî 
Today at 2:51 PM
Excellent answers @Margaret Gathoni @Vijaya Tatavarty üéâ ! We want to minimize k while also minimizing average cluster variance at the same time. We can plot the number of clusters vs the average cluster variance and choose the value of k when adding more clusters doesn't have a significant impact on the cluster variance.

Feb2, 23
How would you assess the quality of the Clustering technique ü•∏ ?

Margaret Gathoni
 ‚Äî 
Yesterday at 1:18 PM
Using evaluation metrics such as Silhouette score, WCSS
Hsin-Wen Chang
 ‚Äî 
Yesterday at 3:07 PM
Awesome answer @Margaret Gathoni üéâ ! Detail reading:
https://www.sciencedirect.com/topics/computer-science/clustering-quality

Feb3, 23
What is Principal Component Analysis?ü§ì

Faith Were
 ‚Äî 
Today at 1:00 PM
PCA is used in Machine Learning for predictive models.
It is a Statistical procedure that uses orthogonal transformation that converts a set of correlated to uncorrelated variables. It is also used in EDA (Exploratory Data Analysis).
hannahkariuki
 ‚Äî 
Today at 1:08 PM
Principal Component Analysis is an unsupervised algorithm for dimensionality reduction of data where most variables are correlated. PCA achieves dimensionality reduction through transforming correlated variables to a new set of variables (PCs)which are uncorrelated such that the first few PCs retain most of the information present in the original variables. The Principal Components (PCs) are a linear combination of the original variables. 
Margaret Gathoni
 ‚Äî 
Today at 1:22 PM
PCA is basically a statistical procedure to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables. Each of the principal components is chosen in such a way so that it would describe most of them still available variance and all these principal components are orthogonal to each other. In all principal components first principal component has a maximum variance.
Kehinde Olalekan
 ‚Äî 
Today at 1:54 PM
Principal Component Analysis (PCA) is a dimensionality-reduction method often used to reduce the dimensionality of large datasets by transforming a large set of variables into a smaller one that still contains most of the information in the large set.
Hsin-Wen Chang
 ‚Äî 
Today at 2:37 PM
Excellent answers @hannahkariuki @Margaret Gathoni @Kehinde Olalekan ü•≥ ! @Faith Were  very close PCA is a dimensionality reduction technique and a data pre-processing technique which transform a high dimensional feature space into smaller components that still retain most of the relevant information like @Margaret Gathoni and @Faith Were said the first component has the maximum variance in the data followed by the second component and so on with the constraint that all the components are orthogonal to each other. 

Feb6, 23
Are Dimensionality Reduction techniques Supervised or Unsupervised learning ü§ì ?

Faith Were
 ‚Äî 
Today at 12:39 PM
Dimensionality reduction is unsupervised learning, this is because they don't use further information of the data such as class labels and can allow an unbiased view of the structure within the data.
Margaret Gathoni
 ‚Äî 
Today at 1:23 PM
Unsupervised learning. However, we can use techniques like LDA in supervised learning for data transformation during pre-processing
hannahkariuki
 ‚Äî 
Today at 1:26 PM
Most dimensionality reduction methods are unsupervised like PCA. But there are other supervised dimensionality reduction methods like NCA
Hsin-Wen Chang
 ‚Äî 
Today at 2:16 PM
Good answers @Margaret Gathoni @hannahkariuki ü•≥ ! Very close @Faith Were üí™ ! In general, we use dimension reduction technique for unsupervised learning tasks but they can also be used in supervised learning such as @Margaret Gathoni said Linear Discriminant Analysis (LDA) which is designed to find low dimensional projection that maximizes the class separation another one like Partial Least Squares (PLS) which looks for the projections having the highest covariance with the group labels. Another one like @hannahkariuki said is Neighborhood Components Analysis (NCA)  which automatically selecting the most significant features that a stochastic nearest neighbor algorithm will give the best accuracy. 

Feb7, 23
List some of the ways of reducing the dimensionality of the given dataset ü§ì

Margaret Gathoni
 ‚Äî 
Today at 12:13 PM
LDA, PCA, Factor Analysis and SVD
Harini Anand
 ‚Äî 
Today at 12:21 PM
Feature selection and Feature Extraction?
hannahkariuki
 ‚Äî 
Today at 12:31 PM
Feature selection which involves selecting a subset of the original features and feature extraction which involves transforming the data from high dimensional space to low dimensional space.
Hsin-Wen Chang
 ‚Äî 
Today at 2:47 PM
Good answers @Margaret Gathoni @Harini Anand @hannahkariuki ü•≥ ! Like @Margaret Gathoni said in PCA features are transformed into a new set of features much lesser compare to the origin which are linear combination of the original features. Forward and Backward Feature Elimination. LDA which we already discussed in previous question. Generalized discriminant analysis (GDA) for nonlinear discriminant analysis using the kernel function operator. Thank you @hannahkariuki for the awesome explanationüöÄ !

Feb8, 23
How would you reduce the dimensionality of a dataset which has correlated features? ü§ì

Margaret Gathoni
 ‚Äî 
Today at 1:51 PM
PCA helps transform correlated feature into uncorrelated features. Feature selection can also be used when you have important features that have high  correlation with the target variable and select a subset of these features. LDA in case of class separation.
Hsin-Wen Chang
 ‚Äî 
Today at 3:37 PM
Good answer @Margaret Gathoni ü•≥ ! We can use the linear correlation node calculates the correlation coefficient for all the pairs of numerical columns in the dataset as the Pearson's Product Moment coefficient and for all pairs of nominal columns as the Pearson's chi square value. We can use it to build the correlation matrix for the features and for all the pairs of columns with correlation higher than a given threshold remove one of the two features.

Feb9, 23
What is the difference between density-sparse data and dimensionally sparse data ü§ì ?

Margaret Gathoni
 ‚Äî 
Today at 2:03 PM
Data that is densely sparse contains a high percentage of null values, and data that is dimensionally sparse has a large feature space with redundant or complimentary features. Learnt this from your daily ML question. Xiexie
Hsin-Wen Chang
 ‚Äî 
Today at 3:48 PM
Good answer @Margaret Gathoni ü•≥ ! One more thing is density-sparse data means high percentage of the data contain null or 0 values. No need to say xie xie to me thank you is enough ü§ì
Oops. I found out I already ask this question before I should change a new one üòÖ . 
Our Machine Learning interview question for the day: Is it beneficial to perform dimensionality reduction before fitting an SVM? Why or why not?ü§ì
'Toba Adesugba
 ‚Äî 
Today at 4:10 PM
I think it is beneficial to perform dimensionality reduction before fitting on SVM because SVM performs better on smaller datasets and reduced features
Hsin-Wen Chang
 ‚Äî 
Today at 5:05 PM
Very close @'Toba Adesugba üí™ ! Reducing the number of features will definitely reduce the computational complexity of the model but it may not improve the performance of SVM since SVM already uses regularization to avoid overfitting that's why performing dimensionality reduction before SVM may not improve the performance of the SVM classifier unless we have a vey large number of features.

